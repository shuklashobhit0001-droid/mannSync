import type { BaseClientOptions, BaseRequestOptions } from "../../../../../../BaseClient";
import * as core from "../../../../../../core";
import * as ElevenLabs from "../../../../../index";
export declare namespace BatchCalls {
    interface Options extends BaseClientOptions {
    }
    interface RequestOptions extends BaseRequestOptions {
    }
}
export declare class BatchCalls {
    protected readonly _options: BatchCalls.Options;
    constructor(_options?: BatchCalls.Options);
    /**
     * Submit a batch call request to schedule calls for multiple recipients.
     *
     * @param {ElevenLabs.conversationalAi.BodySubmitABatchCallRequestV1ConvaiBatchCallingSubmitPost} request
     * @param {BatchCalls.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.batchCalls.create({
     *         callName: "call_name",
     *         agentId: "agent_id",
     *         recipients: [{}]
     *     })
     */
    create(request: ElevenLabs.conversationalAi.BodySubmitABatchCallRequestV1ConvaiBatchCallingSubmitPost, requestOptions?: BatchCalls.RequestOptions): core.HttpResponsePromise<ElevenLabs.BatchCallResponse>;
    private __create;
    /**
     * Get all batch calls for the current workspace.
     *
     * @param {ElevenLabs.conversationalAi.BatchCallsListRequest} request
     * @param {BatchCalls.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.batchCalls.list({
     *         limit: 1,
     *         lastDoc: "last_doc"
     *     })
     */
    list(request?: ElevenLabs.conversationalAi.BatchCallsListRequest, requestOptions?: BatchCalls.RequestOptions): core.HttpResponsePromise<ElevenLabs.WorkspaceBatchCallsResponse>;
    private __list;
    /**
     * Get detailed information about a batch call including all recipients.
     *
     * @param {string} batch_id
     * @param {BatchCalls.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.batchCalls.get("batch_id")
     */
    get(batch_id: string, requestOptions?: BatchCalls.RequestOptions): core.HttpResponsePromise<ElevenLabs.BatchCallDetailedResponse>;
    private __get;
    /**
     * Cancel a running batch call and set all recipients to cancelled status.
     *
     * @param {string} batch_id
     * @param {BatchCalls.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.batchCalls.cancel("batch_id")
     */
    cancel(batch_id: string, requestOptions?: BatchCalls.RequestOptions): core.HttpResponsePromise<ElevenLabs.BatchCallResponse>;
    private __cancel;
    /**
     * Retry a batch call, calling failed and no-response recipients again.
     *
     * @param {string} batch_id
     * @param {BatchCalls.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.batchCalls.retry("batch_id")
     */
    retry(batch_id: string, requestOptions?: BatchCalls.RequestOptions): core.HttpResponsePromise<ElevenLabs.BatchCallResponse>;
    private __retry;
}
